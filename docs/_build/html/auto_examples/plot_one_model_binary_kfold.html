<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Binary: training one kfold model &mdash; fusionlibrary  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/florencestheme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-rendered-html.css" type="text/css" />
    <link rel="shortcut icon" href="../_static/pink_pasta_logo.png"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            fusionlibrary
              <img src="../_static/pink_pasta_logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">ğŸŒ¸ Table of Contents ğŸŒ¸</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">Fusilli: an introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../fusion_model_explanations.html">Fusion Model Explanations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide.html">User guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modifying_models.html">Modifying the fusion models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸŒ¸ Tutorials ğŸŒ¸</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html">Tutorials and Examples Gallery</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ğŸŒ¸ API Reference ğŸŒ¸</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../autosummary/fusionlibrary.fusion_models.html">fusionlibrary.fusion_models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../autosummary/fusionlibrary.datamodules.html">fusionlibrary.datamodules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../autosummary/fusionlibrary.train_functions.html">fusionlibrary.train_functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../autosummary/fusionlibrary.eval_functions.Plotter.html">fusionlibrary.eval_functions.Plotter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../autosummary/fusionlibrary.utils.model_chooser.html">fusionlibrary.utils.model_chooser</a></li>
<li class="toctree-l1"><a class="reference internal" href="../autosummary/fusionlibrary.utils.pl_utils.html">fusionlibrary.utils.pl_utils</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">fusionlibrary</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Binary: training one kfold model</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/auto_examples/plot_one_model_binary_kfold.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-auto-examples-plot-one-model-binary-kfold-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code</p>
</div>
<section class="sphx-glr-example-title" id="binary-training-one-kfold-model">
<span id="sphx-glr-auto-examples-plot-one-model-binary-kfold-py"></span><h1>Binary: training one kfold model<a class="headerlink" href="#binary-training-one-kfold-model" title="Permalink to this heading">ïƒ</a></h1>
<p>This script shows how to train one fusion models on a binary task with k-fold training protocol and multimodal tabular data.</p>
<p>Key Features:</p>
<ul class="simple">
<li><p>Importing a model based on its path.</p></li>
<li><p>Training and testing a model with k-fold cross validation.</p></li>
<li><p>Plotting the results of a single k-fold model.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">docs.examples</span> <span class="kn">import</span> <span class="n">generate_sklearn_simulated_data</span>
<span class="kn">from</span> <span class="nn">fusionlibrary.datamodules</span> <span class="kn">import</span> <span class="n">get_data_module</span>
<span class="kn">from</span> <span class="nn">fusionlibrary.eval_functions</span> <span class="kn">import</span> <span class="n">Plotter</span>
<span class="kn">from</span> <span class="nn">fusionlibrary.fusion_models.base_pl_model</span> <span class="kn">import</span> <span class="n">BaseModel</span>
</pre></div>
</div>
<section id="import-model">
<h2>1. Import model<a class="headerlink" href="#import-model" title="Permalink to this heading">ïƒ</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fusionlibrary.fusion_models.tab_crossmodal_att</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">TabularCrossmodalMultiheadAttention</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">fusionlibrary.train_functions</span> <span class="kn">import</span> <span class="n">train_and_save_models</span>
</pre></div>
</div>
</section>
<section id="set-the-training-parameters">
<h2>2. Set the training parameters<a class="headerlink" href="#set-the-training-parameters" title="Permalink to this heading">ïƒ</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;test_size&quot;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="s2">&quot;kfold_flag&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="s2">&quot;num_k&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
    <span class="s2">&quot;log&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="s2">&quot;pred_type&quot;</span><span class="p">:</span> <span class="s2">&quot;binary&quot;</span><span class="p">,</span>
    <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="generate-simulated-data">
<h2>3. Generate simulated data<a class="headerlink" href="#generate-simulated-data" title="Permalink to this heading">ïƒ</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="n">generate_sklearn_simulated_data</span><span class="p">(</span>
    <span class="n">num_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">num_tab1_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">num_tab2_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">img_dims</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
    <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p>Initialise model</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">fusion_model</span> <span class="o">=</span> <span class="n">TabularCrossmodalMultiheadAttention</span>

<span class="n">single_model_dict</span> <span class="o">=</span> <span class="p">{}</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;method_name:&quot;</span><span class="p">,</span> <span class="n">fusion_model</span><span class="o">.</span><span class="n">method_name</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;modality_type:&quot;</span><span class="p">,</span> <span class="n">fusion_model</span><span class="o">.</span><span class="n">modality_type</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;fusion_type:&quot;</span><span class="p">,</span> <span class="n">fusion_model</span><span class="o">.</span><span class="n">fusion_type</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>method_name: Tabular Crossmodal multi-head attention
modality_type: both_tab
fusion_type: attention
</pre></div>
</div>
</section>
<section id="train-and-test-the-model">
<h2>5. Train and test the model<a class="headerlink" href="#train-and-test-the-model" title="Permalink to this heading">ïƒ</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dm</span> <span class="o">=</span> <span class="n">get_data_module</span><span class="p">(</span>
    <span class="n">fusion_model</span><span class="o">=</span><span class="n">fusion_model</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># train and test</span>
<span class="n">single_model_dict</span> <span class="o">=</span> <span class="n">train_and_save_models</span><span class="p">(</span>
    <span class="n">trained_models_dict</span><span class="o">=</span><span class="n">single_model_dict</span><span class="p">,</span>
    <span class="n">data_module</span><span class="o">=</span><span class="n">dm</span><span class="p">,</span>
    <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span>
    <span class="n">fusion_model</span><span class="o">=</span><span class="n">fusion_model</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Training: 0it [00:00, ?it/s]
Training:   0%|          | 0/17 [00:00&lt;?, ?it/s]
Epoch 0:   0%|          | 0/17 [00:00&lt;?, ?it/s]
Epoch 0:   6%|â–Œ         | 1/17 [00:00&lt;00:00, 18.05it/s]
Epoch 0:   6%|â–Œ         | 1/17 [00:00&lt;00:00, 17.94it/s, loss=0.769]
Epoch 0:  12%|â–ˆâ–        | 2/17 [00:00&lt;00:00, 32.19it/s, loss=0.769]
Epoch 0:  12%|â–ˆâ–        | 2/17 [00:00&lt;00:00, 32.07it/s, loss=0.715]
Epoch 0:  18%|â–ˆâ–Š        | 3/17 [00:00&lt;00:00, 43.19it/s, loss=0.715]
Epoch 0:  18%|â–ˆâ–Š        | 3/17 [00:00&lt;00:00, 43.08it/s, loss=0.722]
Epoch 0:  24%|â–ˆâ–ˆâ–       | 4/17 [00:00&lt;00:00, 52.66it/s, loss=0.722]
Epoch 0:  24%|â–ˆâ–ˆâ–       | 4/17 [00:00&lt;00:00, 52.52it/s, loss=0.715]
Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:00&lt;00:00, 60.62it/s, loss=0.715]
Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:00&lt;00:00, 60.49it/s, loss=0.704]
Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:00&lt;00:00, 66.92it/s, loss=0.704]
Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:00&lt;00:00, 66.78it/s, loss=0.697]
Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:00&lt;00:00, 73.06it/s, loss=0.697]
Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:00&lt;00:00, 72.91it/s, loss=0.698]
Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:00&lt;00:00, 78.31it/s, loss=0.698]
Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:00&lt;00:00, 78.17it/s, loss=0.702]
Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [00:00&lt;00:00, 82.89it/s, loss=0.702]
Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [00:00&lt;00:00, 82.75it/s, loss=0.705]
Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:00&lt;00:00, 87.44it/s, loss=0.705]
Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:00&lt;00:00, 87.30it/s, loss=0.71]
Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:00&lt;00:00, 90.95it/s, loss=0.71]
Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:00&lt;00:00, 90.80it/s, loss=0.71]
Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:00&lt;00:00, 94.39it/s, loss=0.71]
Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:00&lt;00:00, 94.26it/s, loss=0.714]
Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:00&lt;00:00, 97.56it/s, loss=0.714]
Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:00&lt;00:00, 97.44it/s, loss=0.713]
Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:00&lt;00:00, 102.89it/s, loss=0.713]
Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:00&lt;00:00, 109.17it/s, loss=0.713]
Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:00&lt;00:00, 115.43it/s, loss=0.713]
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 121.80it/s, loss=0.713]
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 120.24it/s, loss=0.713, val_loss=0.697]
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 118.44it/s, loss=0.713, val_loss=0.697, train_loss=0.713]
Epoch 0:   0%|          | 0/17 [00:00&lt;?, ?it/s, loss=0.713, val_loss=0.697, train_loss=0.713]
Epoch 1:   0%|          | 0/17 [00:00&lt;?, ?it/s, loss=0.713, val_loss=0.697, train_loss=0.713]
Epoch 1:   6%|â–Œ         | 1/17 [00:00&lt;00:00, 167.64it/s, loss=0.713, val_loss=0.697, train_loss=0.713]
Epoch 1:   6%|â–Œ         | 1/17 [00:00&lt;00:00, 162.53it/s, loss=0.713, val_loss=0.697, train_loss=0.713]
Epoch 1:  12%|â–ˆâ–        | 2/17 [00:00&lt;00:00, 161.90it/s, loss=0.713, val_loss=0.697, train_loss=0.713]
Epoch 1:  12%|â–ˆâ–        | 2/17 [00:00&lt;00:00, 159.16it/s, loss=0.709, val_loss=0.697, train_loss=0.713]
Epoch 1:  18%|â–ˆâ–Š        | 3/17 [00:00&lt;00:00, 163.52it/s, loss=0.709, val_loss=0.697, train_loss=0.713]
Epoch 1:  18%|â–ˆâ–Š        | 3/17 [00:00&lt;00:00, 161.82it/s, loss=0.709, val_loss=0.697, train_loss=0.713]
Epoch 1:  24%|â–ˆâ–ˆâ–       | 4/17 [00:00&lt;00:00, 162.71it/s, loss=0.709, val_loss=0.697, train_loss=0.713]
Epoch 1:  24%|â–ˆâ–ˆâ–       | 4/17 [00:00&lt;00:00, 161.47it/s, loss=0.709, val_loss=0.697, train_loss=0.713]
Epoch 1:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:00&lt;00:00, 161.64it/s, loss=0.709, val_loss=0.697, train_loss=0.713]
Epoch 1:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:00&lt;00:00, 160.60it/s, loss=0.709, val_loss=0.697, train_loss=0.713]
Epoch 1:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:00&lt;00:00, 163.49it/s, loss=0.709, val_loss=0.697, train_loss=0.713]
Epoch 1:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:00&lt;00:00, 162.75it/s, loss=0.707, val_loss=0.697, train_loss=0.713]
Epoch 1:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:00&lt;00:00, 163.57it/s, loss=0.707, val_loss=0.697, train_loss=0.713]
Epoch 1:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:00&lt;00:00, 162.87it/s, loss=0.706, val_loss=0.697, train_loss=0.713]
Epoch 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:00&lt;00:00, 164.28it/s, loss=0.706, val_loss=0.697, train_loss=0.713]
Epoch 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:00&lt;00:00, 163.69it/s, loss=0.702, val_loss=0.697, train_loss=0.713]
Epoch 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [00:00&lt;00:00, 165.75it/s, loss=0.702, val_loss=0.697, train_loss=0.713]
Epoch 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [00:00&lt;00:00, 165.29it/s, loss=0.703, val_loss=0.697, train_loss=0.713]
Epoch 1:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:00&lt;00:00, 166.22it/s, loss=0.703, val_loss=0.697, train_loss=0.713]
Epoch 1:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:00&lt;00:00, 165.75it/s, loss=0.701, val_loss=0.697, train_loss=0.713]
Epoch 1:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:00&lt;00:00, 167.42it/s, loss=0.701, val_loss=0.697, train_loss=0.713]
Epoch 1:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:00&lt;00:00, 166.98it/s, loss=0.701, val_loss=0.697, train_loss=0.713]
Epoch 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:00&lt;00:00, 166.83it/s, loss=0.701, val_loss=0.697, train_loss=0.713]
Epoch 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:00&lt;00:00, 166.43it/s, loss=0.702, val_loss=0.697, train_loss=0.713]
Epoch 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:00&lt;00:00, 168.11it/s, loss=0.702, val_loss=0.697, train_loss=0.713]
Epoch 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:00&lt;00:00, 167.78it/s, loss=0.704, val_loss=0.697, train_loss=0.713]
Epoch 1:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:00&lt;00:00, 175.15it/s, loss=0.704, val_loss=0.697, train_loss=0.713]
Epoch 1:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:00&lt;00:00, 184.97it/s, loss=0.704, val_loss=0.697, train_loss=0.713]
Epoch 1:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:00&lt;00:00, 194.61it/s, loss=0.704, val_loss=0.697, train_loss=0.713]
Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 204.59it/s, loss=0.704, val_loss=0.697, train_loss=0.713]
Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 201.93it/s, loss=0.704, val_loss=0.685, train_loss=0.713]
Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 200.93it/s, loss=0.704, val_loss=0.685, train_loss=0.691]
Epoch 1:   0%|          | 0/17 [00:00&lt;?, ?it/s, loss=0.704, val_loss=0.685, train_loss=0.691]
Epoch 2:   0%|          | 0/17 [00:00&lt;?, ?it/s, loss=0.704, val_loss=0.685, train_loss=0.691]
Epoch 2:   6%|â–Œ         | 1/17 [00:00&lt;00:00, 146.74it/s, loss=0.704, val_loss=0.685, train_loss=0.691]
Epoch 2:   6%|â–Œ         | 1/17 [00:00&lt;00:00, 142.92it/s, loss=0.703, val_loss=0.685, train_loss=0.691]
Epoch 2:  12%|â–ˆâ–        | 2/17 [00:00&lt;00:00, 150.82it/s, loss=0.703, val_loss=0.685, train_loss=0.691]
Epoch 2:  12%|â–ˆâ–        | 2/17 [00:00&lt;00:00, 148.88it/s, loss=0.701, val_loss=0.685, train_loss=0.691]
Epoch 2:  18%|â–ˆâ–Š        | 3/17 [00:00&lt;00:00, 152.63it/s, loss=0.701, val_loss=0.685, train_loss=0.691]
Epoch 2:  18%|â–ˆâ–Š        | 3/17 [00:00&lt;00:00, 151.25it/s, loss=0.699, val_loss=0.685, train_loss=0.691]
Epoch 2:  24%|â–ˆâ–ˆâ–       | 4/17 [00:00&lt;00:00, 158.25it/s, loss=0.699, val_loss=0.685, train_loss=0.691]
Epoch 2:  24%|â–ˆâ–ˆâ–       | 4/17 [00:00&lt;00:00, 157.03it/s, loss=0.695, val_loss=0.685, train_loss=0.691]
Epoch 2:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:00&lt;00:00, 159.28it/s, loss=0.695, val_loss=0.685, train_loss=0.691]
Epoch 2:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:00&lt;00:00, 158.29it/s, loss=0.693, val_loss=0.685, train_loss=0.691]
Epoch 2:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:00&lt;00:00, 159.23it/s, loss=0.693, val_loss=0.685, train_loss=0.691]
Epoch 2:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:00&lt;00:00, 158.46it/s, loss=0.689, val_loss=0.685, train_loss=0.691]
Epoch 2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:00&lt;00:00, 160.42it/s, loss=0.689, val_loss=0.685, train_loss=0.691]
Epoch 2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:00&lt;00:00, 159.84it/s, loss=0.688, val_loss=0.685, train_loss=0.691]
Epoch 2:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:00&lt;00:00, 162.96it/s, loss=0.688, val_loss=0.685, train_loss=0.691]
Epoch 2:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:00&lt;00:00, 162.38it/s, loss=0.685, val_loss=0.685, train_loss=0.691]
Epoch 2:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [00:00&lt;00:00, 163.76it/s, loss=0.685, val_loss=0.685, train_loss=0.691]
Epoch 2:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [00:00&lt;00:00, 163.25it/s, loss=0.686, val_loss=0.685, train_loss=0.691]
Epoch 2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:00&lt;00:00, 163.49it/s, loss=0.686, val_loss=0.685, train_loss=0.691]
Epoch 2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:00&lt;00:00, 162.98it/s, loss=0.685, val_loss=0.685, train_loss=0.691]
Epoch 2:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:00&lt;00:00, 163.49it/s, loss=0.685, val_loss=0.685, train_loss=0.691]
Epoch 2:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:00&lt;00:00, 163.07it/s, loss=0.682, val_loss=0.685, train_loss=0.691]
Epoch 2:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:00&lt;00:00, 163.62it/s, loss=0.682, val_loss=0.685, train_loss=0.691]
Epoch 2:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:00&lt;00:00, 163.20it/s, loss=0.679, val_loss=0.685, train_loss=0.691]
Epoch 2:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:00&lt;00:00, 164.05it/s, loss=0.679, val_loss=0.685, train_loss=0.691]
Epoch 2:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:00&lt;00:00, 163.70it/s, loss=0.681, val_loss=0.685, train_loss=0.691]
Epoch 2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:00&lt;00:00, 171.05it/s, loss=0.681, val_loss=0.685, train_loss=0.691]
Epoch 2:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:00&lt;00:00, 180.76it/s, loss=0.681, val_loss=0.685, train_loss=0.691]
Epoch 2:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:00&lt;00:00, 190.30it/s, loss=0.681, val_loss=0.685, train_loss=0.691]
Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 200.16it/s, loss=0.681, val_loss=0.685, train_loss=0.691]
Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 197.63it/s, loss=0.681, val_loss=0.658, train_loss=0.691]
Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 196.75it/s, loss=0.681, val_loss=0.658, train_loss=0.675]
Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 185.02it/s, loss=0.681, val_loss=0.658, train_loss=0.675]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     Validate metric           DataLoader 0
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   binary_accuracy_val      0.49000000953674316
    binary_auroc_val        0.8143256902694702
        val_loss            0.6584568023681641
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Training: 0it [00:00, ?it/s]
Training:   0%|          | 0/17 [00:00&lt;?, ?it/s]
Epoch 0:   0%|          | 0/17 [00:00&lt;?, ?it/s]
Epoch 0:   6%|â–Œ         | 1/17 [00:00&lt;00:00, 138.74it/s]
Epoch 0:   6%|â–Œ         | 1/17 [00:00&lt;00:00, 135.05it/s, loss=0.662]
Epoch 0:  12%|â–ˆâ–        | 2/17 [00:00&lt;00:00, 145.42it/s, loss=0.662]
Epoch 0:  12%|â–ˆâ–        | 2/17 [00:00&lt;00:00, 143.21it/s, loss=0.707]
Epoch 0:  18%|â–ˆâ–Š        | 3/17 [00:00&lt;00:00, 152.84it/s, loss=0.707]
Epoch 0:  18%|â–ˆâ–Š        | 3/17 [00:00&lt;00:00, 151.35it/s, loss=0.727]
Epoch 0:  24%|â–ˆâ–ˆâ–       | 4/17 [00:00&lt;00:00, 153.81it/s, loss=0.727]
Epoch 0:  24%|â–ˆâ–ˆâ–       | 4/17 [00:00&lt;00:00, 152.73it/s, loss=0.721]
Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:00&lt;00:00, 157.26it/s, loss=0.721]
Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:00&lt;00:00, 156.40it/s, loss=0.724]
Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:00&lt;00:00, 159.37it/s, loss=0.724]
Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:00&lt;00:00, 158.59it/s, loss=0.734]
Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:00&lt;00:00, 159.94it/s, loss=0.734]
Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:00&lt;00:00, 159.35it/s, loss=0.734]
Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:00&lt;00:00, 160.14it/s, loss=0.734]
Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:00&lt;00:00, 159.50it/s, loss=0.733]
Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [00:00&lt;00:00, 158.06it/s, loss=0.733]
Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [00:00&lt;00:00, 157.52it/s, loss=0.719]
Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:00&lt;00:00, 158.53it/s, loss=0.719]
Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:00&lt;00:00, 158.00it/s, loss=0.719]
Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:00&lt;00:00, 158.39it/s, loss=0.719]
Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:00&lt;00:00, 157.96it/s, loss=0.712]
Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:00&lt;00:00, 157.85it/s, loss=0.712]
Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:00&lt;00:00, 157.46it/s, loss=0.711]
Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:00&lt;00:00, 158.26it/s, loss=0.711]
Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:00&lt;00:00, 157.93it/s, loss=0.716]
Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:00&lt;00:00, 165.52it/s, loss=0.716]
Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:00&lt;00:00, 174.07it/s, loss=0.716]
Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:00&lt;00:00, 182.69it/s, loss=0.716]
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 191.05it/s, loss=0.716]
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 187.12it/s, loss=0.716, val_loss=0.701]
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 186.20it/s, loss=0.716, val_loss=0.701, train_loss=0.713]
Epoch 0:   0%|          | 0/17 [00:00&lt;?, ?it/s, loss=0.716, val_loss=0.701, train_loss=0.713]
Epoch 1:   0%|          | 0/17 [00:00&lt;?, ?it/s, loss=0.716, val_loss=0.701, train_loss=0.713]
Epoch 1:   6%|â–Œ         | 1/17 [00:00&lt;00:00, 162.82it/s, loss=0.716, val_loss=0.701, train_loss=0.713]
Epoch 1:   6%|â–Œ         | 1/17 [00:00&lt;00:00, 158.18it/s, loss=0.717, val_loss=0.701, train_loss=0.713]
Epoch 1:  12%|â–ˆâ–        | 2/17 [00:00&lt;00:00, 167.31it/s, loss=0.717, val_loss=0.701, train_loss=0.713]
Epoch 1:  12%|â–ˆâ–        | 2/17 [00:00&lt;00:00, 164.23it/s, loss=0.714, val_loss=0.701, train_loss=0.713]
Epoch 1:  18%|â–ˆâ–Š        | 3/17 [00:00&lt;00:00, 167.85it/s, loss=0.714, val_loss=0.701, train_loss=0.713]
Epoch 1:  18%|â–ˆâ–Š        | 3/17 [00:00&lt;00:00, 166.22it/s, loss=0.715, val_loss=0.701, train_loss=0.713]
Epoch 1:  24%|â–ˆâ–ˆâ–       | 4/17 [00:00&lt;00:00, 167.41it/s, loss=0.715, val_loss=0.701, train_loss=0.713]
Epoch 1:  24%|â–ˆâ–ˆâ–       | 4/17 [00:00&lt;00:00, 166.05it/s, loss=0.713, val_loss=0.701, train_loss=0.713]
Epoch 1:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:00&lt;00:00, 165.57it/s, loss=0.713, val_loss=0.701, train_loss=0.713]
Epoch 1:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:00&lt;00:00, 164.64it/s, loss=0.711, val_loss=0.701, train_loss=0.713]
Epoch 1:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:00&lt;00:00, 166.49it/s, loss=0.711, val_loss=0.701, train_loss=0.713]
Epoch 1:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:00&lt;00:00, 165.60it/s, loss=0.709, val_loss=0.701, train_loss=0.713]
Epoch 1:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:00&lt;00:00, 165.73it/s, loss=0.709, val_loss=0.701, train_loss=0.713]
Epoch 1:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:00&lt;00:00, 165.12it/s, loss=0.709, val_loss=0.701, train_loss=0.713]
Epoch 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:00&lt;00:00, 167.21it/s, loss=0.709, val_loss=0.701, train_loss=0.713]
Epoch 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:00&lt;00:00, 166.71it/s, loss=0.71, val_loss=0.701, train_loss=0.713]
Epoch 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [00:00&lt;00:00, 167.69it/s, loss=0.71, val_loss=0.701, train_loss=0.713]
Epoch 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [00:00&lt;00:00, 167.21it/s, loss=0.707, val_loss=0.701, train_loss=0.713]
Epoch 1:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:00&lt;00:00, 168.46it/s, loss=0.707, val_loss=0.701, train_loss=0.713]
Epoch 1:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:00&lt;00:00, 167.97it/s, loss=0.702, val_loss=0.701, train_loss=0.713]
Epoch 1:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:00&lt;00:00, 169.70it/s, loss=0.702, val_loss=0.701, train_loss=0.713]
Epoch 1:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:00&lt;00:00, 169.28it/s, loss=0.701, val_loss=0.701, train_loss=0.713]
Epoch 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:00&lt;00:00, 170.15it/s, loss=0.701, val_loss=0.701, train_loss=0.713]
Epoch 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:00&lt;00:00, 169.77it/s, loss=0.698, val_loss=0.701, train_loss=0.713]
Epoch 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:00&lt;00:00, 170.80it/s, loss=0.698, val_loss=0.701, train_loss=0.713]
Epoch 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:00&lt;00:00, 170.44it/s, loss=0.693, val_loss=0.701, train_loss=0.713]
Epoch 1:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:00&lt;00:00, 177.63it/s, loss=0.693, val_loss=0.701, train_loss=0.713]
Epoch 1:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:00&lt;00:00, 187.14it/s, loss=0.693, val_loss=0.701, train_loss=0.713]
Epoch 1:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:00&lt;00:00, 196.55it/s, loss=0.693, val_loss=0.701, train_loss=0.713]
Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 206.29it/s, loss=0.693, val_loss=0.701, train_loss=0.713]
Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 203.33it/s, loss=0.693, val_loss=0.679, train_loss=0.713]
Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 202.30it/s, loss=0.693, val_loss=0.679, train_loss=0.689]
Epoch 1:   0%|          | 0/17 [00:00&lt;?, ?it/s, loss=0.693, val_loss=0.679, train_loss=0.689]
Epoch 2:   0%|          | 0/17 [00:00&lt;?, ?it/s, loss=0.693, val_loss=0.679, train_loss=0.689]
Epoch 2:   6%|â–Œ         | 1/17 [00:00&lt;00:00, 161.13it/s, loss=0.693, val_loss=0.679, train_loss=0.689]
Epoch 2:   6%|â–Œ         | 1/17 [00:00&lt;00:00, 156.07it/s, loss=0.69, val_loss=0.679, train_loss=0.689]
Epoch 2:  12%|â–ˆâ–        | 2/17 [00:00&lt;00:00, 163.30it/s, loss=0.69, val_loss=0.679, train_loss=0.689]
Epoch 2:  12%|â–ˆâ–        | 2/17 [00:00&lt;00:00, 160.94it/s, loss=0.687, val_loss=0.679, train_loss=0.689]
Epoch 2:  18%|â–ˆâ–Š        | 3/17 [00:00&lt;00:00, 163.61it/s, loss=0.687, val_loss=0.679, train_loss=0.689]
Epoch 2:  18%|â–ˆâ–Š        | 3/17 [00:00&lt;00:00, 161.86it/s, loss=0.69, val_loss=0.679, train_loss=0.689]
Epoch 2:  24%|â–ˆâ–ˆâ–       | 4/17 [00:00&lt;00:00, 161.86it/s, loss=0.69, val_loss=0.679, train_loss=0.689]
Epoch 2:  24%|â–ˆâ–ˆâ–       | 4/17 [00:00&lt;00:00, 160.62it/s, loss=0.688, val_loss=0.679, train_loss=0.689]
Epoch 2:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:00&lt;00:00, 158.88it/s, loss=0.688, val_loss=0.679, train_loss=0.689]
Epoch 2:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:00&lt;00:00, 157.88it/s, loss=0.688, val_loss=0.679, train_loss=0.689]
Epoch 2:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:00&lt;00:00, 161.07it/s, loss=0.688, val_loss=0.679, train_loss=0.689]
Epoch 2:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:00&lt;00:00, 160.37it/s, loss=0.687, val_loss=0.679, train_loss=0.689]
Epoch 2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:00&lt;00:00, 162.13it/s, loss=0.687, val_loss=0.679, train_loss=0.689]
Epoch 2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:00&lt;00:00, 161.54it/s, loss=0.684, val_loss=0.679, train_loss=0.689]
Epoch 2:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:00&lt;00:00, 162.91it/s, loss=0.684, val_loss=0.679, train_loss=0.689]
Epoch 2:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:00&lt;00:00, 162.41it/s, loss=0.68, val_loss=0.679, train_loss=0.689]
Epoch 2:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [00:00&lt;00:00, 164.72it/s, loss=0.68, val_loss=0.679, train_loss=0.689]
Epoch 2:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [00:00&lt;00:00, 164.27it/s, loss=0.677, val_loss=0.679, train_loss=0.689]
Epoch 2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:00&lt;00:00, 166.40it/s, loss=0.677, val_loss=0.679, train_loss=0.689]
Epoch 2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:00&lt;00:00, 165.99it/s, loss=0.673, val_loss=0.679, train_loss=0.689]
Epoch 2:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:00&lt;00:00, 168.21it/s, loss=0.673, val_loss=0.679, train_loss=0.689]
Epoch 2:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:00&lt;00:00, 167.82it/s, loss=0.67, val_loss=0.679, train_loss=0.689]
Epoch 2:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:00&lt;00:00, 169.80it/s, loss=0.67, val_loss=0.679, train_loss=0.689]
Epoch 2:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:00&lt;00:00, 169.44it/s, loss=0.669, val_loss=0.679, train_loss=0.689]
Epoch 2:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:00&lt;00:00, 171.12it/s, loss=0.669, val_loss=0.679, train_loss=0.689]
Epoch 2:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:00&lt;00:00, 170.80it/s, loss=0.667, val_loss=0.679, train_loss=0.689]
Epoch 2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:00&lt;00:00, 178.48it/s, loss=0.667, val_loss=0.679, train_loss=0.689]
Epoch 2:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:00&lt;00:00, 188.44it/s, loss=0.667, val_loss=0.679, train_loss=0.689]
Epoch 2:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:00&lt;00:00, 198.14it/s, loss=0.667, val_loss=0.679, train_loss=0.689]
Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 208.03it/s, loss=0.667, val_loss=0.679, train_loss=0.689]
Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 205.29it/s, loss=0.667, val_loss=0.662, train_loss=0.689]
Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 204.29it/s, loss=0.667, val_loss=0.662, train_loss=0.658]
Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 191.91it/s, loss=0.667, val_loss=0.662, train_loss=0.658]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     Validate metric           DataLoader 0
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   binary_accuracy_val      0.5899999737739563
    binary_auroc_val        0.7444000244140625
        val_loss            0.6621554493904114
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Training: 0it [00:00, ?it/s]
Training:   0%|          | 0/17 [00:00&lt;?, ?it/s]
Epoch 0:   0%|          | 0/17 [00:00&lt;?, ?it/s]
Epoch 0:   6%|â–Œ         | 1/17 [00:00&lt;00:00, 122.88it/s]
Epoch 0:   6%|â–Œ         | 1/17 [00:00&lt;00:00, 119.84it/s, loss=0.708]
Epoch 0:  12%|â–ˆâ–        | 2/17 [00:00&lt;00:00, 134.53it/s, loss=0.708]
Epoch 0:  12%|â–ˆâ–        | 2/17 [00:00&lt;00:00, 132.66it/s, loss=0.73]
Epoch 0:  18%|â–ˆâ–Š        | 3/17 [00:00&lt;00:00, 142.99it/s, loss=0.73]
Epoch 0:  18%|â–ˆâ–Š        | 3/17 [00:00&lt;00:00, 141.82it/s, loss=0.736]
Epoch 0:  24%|â–ˆâ–ˆâ–       | 4/17 [00:00&lt;00:00, 148.52it/s, loss=0.736]
Epoch 0:  24%|â–ˆâ–ˆâ–       | 4/17 [00:00&lt;00:00, 147.48it/s, loss=0.743]
Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:00&lt;00:00, 152.99it/s, loss=0.743]
Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:00&lt;00:00, 152.15it/s, loss=0.73]
Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:00&lt;00:00, 156.54it/s, loss=0.73]
Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:00&lt;00:00, 155.89it/s, loss=0.725]
Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:00&lt;00:00, 158.94it/s, loss=0.725]
Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:00&lt;00:00, 158.40it/s, loss=0.72]
Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:00&lt;00:00, 161.33it/s, loss=0.72]
Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:00&lt;00:00, 160.80it/s, loss=0.72]
Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [00:00&lt;00:00, 162.36it/s, loss=0.72]
Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [00:00&lt;00:00, 161.79it/s, loss=0.72]
Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:00&lt;00:00, 162.38it/s, loss=0.72]
Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:00&lt;00:00, 161.92it/s, loss=0.718]
Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:00&lt;00:00, 160.46it/s, loss=0.718]
Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:00&lt;00:00, 159.70it/s, loss=0.717]
Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:00&lt;00:00, 159.60it/s, loss=0.717]
Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:00&lt;00:00, 159.15it/s, loss=0.713]
Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:00&lt;00:00, 160.52it/s, loss=0.713]
Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:00&lt;00:00, 160.17it/s, loss=0.708]
Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:00&lt;00:00, 167.18it/s, loss=0.708]
Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:00&lt;00:00, 176.46it/s, loss=0.708]
Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:00&lt;00:00, 185.63it/s, loss=0.708]
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 195.10it/s, loss=0.708]
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 191.35it/s, loss=0.708, val_loss=0.700]
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 190.45it/s, loss=0.708, val_loss=0.700, train_loss=0.711]
Epoch 0:   0%|          | 0/17 [00:00&lt;?, ?it/s, loss=0.708, val_loss=0.700, train_loss=0.711]
Epoch 1:   0%|          | 0/17 [00:00&lt;?, ?it/s, loss=0.708, val_loss=0.700, train_loss=0.711]
Epoch 1:   6%|â–Œ         | 1/17 [00:00&lt;00:00, 160.49it/s, loss=0.708, val_loss=0.700, train_loss=0.711]
Epoch 1:   6%|â–Œ         | 1/17 [00:00&lt;00:00, 156.22it/s, loss=0.708, val_loss=0.700, train_loss=0.711]
Epoch 1:  12%|â–ˆâ–        | 2/17 [00:00&lt;00:00, 170.66it/s, loss=0.708, val_loss=0.700, train_loss=0.711]
Epoch 1:  12%|â–ˆâ–        | 2/17 [00:00&lt;00:00, 168.25it/s, loss=0.706, val_loss=0.700, train_loss=0.711]
Epoch 1:  18%|â–ˆâ–Š        | 3/17 [00:00&lt;00:00, 172.76it/s, loss=0.706, val_loss=0.700, train_loss=0.711]
Epoch 1:  18%|â–ˆâ–Š        | 3/17 [00:00&lt;00:00, 171.06it/s, loss=0.705, val_loss=0.700, train_loss=0.711]
Epoch 1:  24%|â–ˆâ–ˆâ–       | 4/17 [00:00&lt;00:00, 172.41it/s, loss=0.705, val_loss=0.700, train_loss=0.711]
Epoch 1:  24%|â–ˆâ–ˆâ–       | 4/17 [00:00&lt;00:00, 171.14it/s, loss=0.703, val_loss=0.700, train_loss=0.711]
Epoch 1:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:00&lt;00:00, 172.76it/s, loss=0.703, val_loss=0.700, train_loss=0.711]
Epoch 1:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:00&lt;00:00, 171.80it/s, loss=0.703, val_loss=0.700, train_loss=0.711]
Epoch 1:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:00&lt;00:00, 172.91it/s, loss=0.703, val_loss=0.700, train_loss=0.711]
Epoch 1:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:00&lt;00:00, 172.12it/s, loss=0.702, val_loss=0.700, train_loss=0.711]
Epoch 1:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:00&lt;00:00, 174.24it/s, loss=0.702, val_loss=0.700, train_loss=0.711]
Epoch 1:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:00&lt;00:00, 173.57it/s, loss=0.701, val_loss=0.700, train_loss=0.711]
Epoch 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:00&lt;00:00, 174.40it/s, loss=0.701, val_loss=0.700, train_loss=0.711]
Epoch 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:00&lt;00:00, 173.71it/s, loss=0.7, val_loss=0.700, train_loss=0.711]
Epoch 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [00:00&lt;00:00, 173.15it/s, loss=0.7, val_loss=0.700, train_loss=0.711]
Epoch 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [00:00&lt;00:00, 172.52it/s, loss=0.696, val_loss=0.700, train_loss=0.711]
Epoch 1:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:00&lt;00:00, 172.54it/s, loss=0.696, val_loss=0.700, train_loss=0.711]
Epoch 1:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:00&lt;00:00, 172.02it/s, loss=0.693, val_loss=0.700, train_loss=0.711]
Epoch 1:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:00&lt;00:00, 173.20it/s, loss=0.693, val_loss=0.700, train_loss=0.711]
Epoch 1:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:00&lt;00:00, 172.76it/s, loss=0.689, val_loss=0.700, train_loss=0.711]
Epoch 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:00&lt;00:00, 171.57it/s, loss=0.689, val_loss=0.700, train_loss=0.711]
Epoch 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:00&lt;00:00, 171.06it/s, loss=0.689, val_loss=0.700, train_loss=0.711]
Epoch 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:00&lt;00:00, 170.85it/s, loss=0.689, val_loss=0.700, train_loss=0.711]
Epoch 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:00&lt;00:00, 170.42it/s, loss=0.687, val_loss=0.700, train_loss=0.711]
Epoch 1:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:00&lt;00:00, 177.65it/s, loss=0.687, val_loss=0.700, train_loss=0.711]
Epoch 1:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:00&lt;00:00, 187.29it/s, loss=0.687, val_loss=0.700, train_loss=0.711]
Epoch 1:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:00&lt;00:00, 196.70it/s, loss=0.687, val_loss=0.700, train_loss=0.711]
Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 206.38it/s, loss=0.687, val_loss=0.700, train_loss=0.711]
Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 203.20it/s, loss=0.687, val_loss=0.683, train_loss=0.711]
Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 202.06it/s, loss=0.687, val_loss=0.683, train_loss=0.684]
Epoch 1:   0%|          | 0/17 [00:00&lt;?, ?it/s, loss=0.687, val_loss=0.683, train_loss=0.684]
Epoch 2:   0%|          | 0/17 [00:00&lt;?, ?it/s, loss=0.687, val_loss=0.683, train_loss=0.684]
Epoch 2:   6%|â–Œ         | 1/17 [00:00&lt;00:00, 143.13it/s, loss=0.687, val_loss=0.683, train_loss=0.684]
Epoch 2:   6%|â–Œ         | 1/17 [00:00&lt;00:00, 137.89it/s, loss=0.685, val_loss=0.683, train_loss=0.684]
Epoch 2:  12%|â–ˆâ–        | 2/17 [00:00&lt;00:00, 141.81it/s, loss=0.685, val_loss=0.683, train_loss=0.684]
Epoch 2:  12%|â–ˆâ–        | 2/17 [00:00&lt;00:00, 139.49it/s, loss=0.682, val_loss=0.683, train_loss=0.684]
Epoch 2:  18%|â–ˆâ–Š        | 3/17 [00:00&lt;00:00, 148.06it/s, loss=0.682, val_loss=0.683, train_loss=0.684]
Epoch 2:  18%|â–ˆâ–Š        | 3/17 [00:00&lt;00:00, 146.67it/s, loss=0.679, val_loss=0.683, train_loss=0.684]
Epoch 2:  24%|â–ˆâ–ˆâ–       | 4/17 [00:00&lt;00:00, 148.94it/s, loss=0.679, val_loss=0.683, train_loss=0.684]
Epoch 2:  24%|â–ˆâ–ˆâ–       | 4/17 [00:00&lt;00:00, 147.51it/s, loss=0.677, val_loss=0.683, train_loss=0.684]
Epoch 2:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:00&lt;00:00, 149.87it/s, loss=0.677, val_loss=0.683, train_loss=0.684]
Epoch 2:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:00&lt;00:00, 148.99it/s, loss=0.677, val_loss=0.683, train_loss=0.684]
Epoch 2:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:00&lt;00:00, 151.70it/s, loss=0.677, val_loss=0.683, train_loss=0.684]
Epoch 2:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:00&lt;00:00, 151.08it/s, loss=0.676, val_loss=0.683, train_loss=0.684]
Epoch 2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:00&lt;00:00, 154.89it/s, loss=0.676, val_loss=0.683, train_loss=0.684]
Epoch 2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:00&lt;00:00, 154.31it/s, loss=0.676, val_loss=0.683, train_loss=0.684]
Epoch 2:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:00&lt;00:00, 157.59it/s, loss=0.676, val_loss=0.683, train_loss=0.684]
Epoch 2:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:00&lt;00:00, 157.01it/s, loss=0.674, val_loss=0.683, train_loss=0.684]
Epoch 2:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [00:00&lt;00:00, 157.26it/s, loss=0.674, val_loss=0.683, train_loss=0.684]
Epoch 2:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [00:00&lt;00:00, 156.75it/s, loss=0.672, val_loss=0.683, train_loss=0.684]
Epoch 2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:00&lt;00:00, 158.86it/s, loss=0.672, val_loss=0.683, train_loss=0.684]
Epoch 2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:00&lt;00:00, 158.46it/s, loss=0.671, val_loss=0.683, train_loss=0.684]
Epoch 2:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:00&lt;00:00, 159.85it/s, loss=0.671, val_loss=0.683, train_loss=0.684]
Epoch 2:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:00&lt;00:00, 159.47it/s, loss=0.67, val_loss=0.683, train_loss=0.684]
Epoch 2:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:00&lt;00:00, 160.96it/s, loss=0.67, val_loss=0.683, train_loss=0.684]
Epoch 2:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:00&lt;00:00, 160.52it/s, loss=0.668, val_loss=0.683, train_loss=0.684]
Epoch 2:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:00&lt;00:00, 160.14it/s, loss=0.668, val_loss=0.683, train_loss=0.684]
Epoch 2:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:00&lt;00:00, 159.77it/s, loss=0.665, val_loss=0.683, train_loss=0.684]
Epoch 2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:00&lt;00:00, 166.47it/s, loss=0.665, val_loss=0.683, train_loss=0.684]
Epoch 2:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:00&lt;00:00, 175.68it/s, loss=0.665, val_loss=0.683, train_loss=0.684]
Epoch 2:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:00&lt;00:00, 184.78it/s, loss=0.665, val_loss=0.683, train_loss=0.684]
Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 194.17it/s, loss=0.665, val_loss=0.683, train_loss=0.684]
Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 191.48it/s, loss=0.665, val_loss=0.659, train_loss=0.684]
Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 190.57it/s, loss=0.665, val_loss=0.659, train_loss=0.660]
Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 179.36it/s, loss=0.665, val_loss=0.659, train_loss=0.660]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     Validate metric           DataLoader 0
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   binary_accuracy_val      0.5199999809265137
    binary_auroc_val        0.8129006028175354
        val_loss             0.658663809299469
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Training: 0it [00:00, ?it/s]
Training:   0%|          | 0/17 [00:00&lt;?, ?it/s]
Epoch 0:   0%|          | 0/17 [00:00&lt;?, ?it/s]
Epoch 0:   6%|â–Œ         | 1/17 [00:00&lt;00:00, 127.12it/s]
Epoch 0:   6%|â–Œ         | 1/17 [00:00&lt;00:00, 122.70it/s, loss=0.678]
Epoch 0:  12%|â–ˆâ–        | 2/17 [00:00&lt;00:00, 138.77it/s, loss=0.678]
Epoch 0:  12%|â–ˆâ–        | 2/17 [00:00&lt;00:00, 136.91it/s, loss=0.709]
Epoch 0:  18%|â–ˆâ–Š        | 3/17 [00:00&lt;00:00, 134.22it/s, loss=0.709]
Epoch 0:  18%|â–ˆâ–Š        | 3/17 [00:00&lt;00:00, 132.81it/s, loss=0.714]
Epoch 0:  24%|â–ˆâ–ˆâ–       | 4/17 [00:00&lt;00:00, 131.47it/s, loss=0.714]
Epoch 0:  24%|â–ˆâ–ˆâ–       | 4/17 [00:00&lt;00:00, 130.43it/s, loss=0.72]
Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:00&lt;00:00, 133.37it/s, loss=0.72]
Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:00&lt;00:00, 132.66it/s, loss=0.72]
Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:00&lt;00:00, 137.47it/s, loss=0.72]
Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:00&lt;00:00, 136.76it/s, loss=0.725]
Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:00&lt;00:00, 138.69it/s, loss=0.725]
Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:00&lt;00:00, 138.12it/s, loss=0.724]
Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:00&lt;00:00, 141.86it/s, loss=0.724]
Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:00&lt;00:00, 141.39it/s, loss=0.718]
Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [00:00&lt;00:00, 143.67it/s, loss=0.718]
Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [00:00&lt;00:00, 143.26it/s, loss=0.727]
Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:00&lt;00:00, 145.43it/s, loss=0.727]
Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:00&lt;00:00, 145.09it/s, loss=0.721]
Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:00&lt;00:00, 147.27it/s, loss=0.721]
Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:00&lt;00:00, 146.93it/s, loss=0.722]
Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:00&lt;00:00, 148.79it/s, loss=0.722]
Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:00&lt;00:00, 148.45it/s, loss=0.722]
Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:00&lt;00:00, 150.35it/s, loss=0.722]
Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:00&lt;00:00, 150.04it/s, loss=0.715]
Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:00&lt;00:00, 157.62it/s, loss=0.715]
Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:00&lt;00:00, 166.64it/s, loss=0.715]
Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:00&lt;00:00, 175.55it/s, loss=0.715]
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 184.77it/s, loss=0.715]
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 181.63it/s, loss=0.715, val_loss=0.692]
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 180.88it/s, loss=0.715, val_loss=0.692, train_loss=0.718]
Epoch 0:   0%|          | 0/17 [00:00&lt;?, ?it/s, loss=0.715, val_loss=0.692, train_loss=0.718]
Epoch 1:   0%|          | 0/17 [00:00&lt;?, ?it/s, loss=0.715, val_loss=0.692, train_loss=0.718]
Epoch 1:   6%|â–Œ         | 1/17 [00:00&lt;00:00, 157.51it/s, loss=0.715, val_loss=0.692, train_loss=0.718]
Epoch 1:   6%|â–Œ         | 1/17 [00:00&lt;00:00, 153.12it/s, loss=0.715, val_loss=0.692, train_loss=0.718]
Epoch 1:  12%|â–ˆâ–        | 2/17 [00:00&lt;00:00, 168.76it/s, loss=0.715, val_loss=0.692, train_loss=0.718]
Epoch 1:  12%|â–ˆâ–        | 2/17 [00:00&lt;00:00, 166.46it/s, loss=0.713, val_loss=0.692, train_loss=0.718]
Epoch 1:  18%|â–ˆâ–Š        | 3/17 [00:00&lt;00:00, 173.63it/s, loss=0.713, val_loss=0.692, train_loss=0.718]
Epoch 1:  18%|â–ˆâ–Š        | 3/17 [00:00&lt;00:00, 171.91it/s, loss=0.713, val_loss=0.692, train_loss=0.718]
Epoch 1:  24%|â–ˆâ–ˆâ–       | 4/17 [00:00&lt;00:00, 171.37it/s, loss=0.713, val_loss=0.692, train_loss=0.718]
Epoch 1:  24%|â–ˆâ–ˆâ–       | 4/17 [00:00&lt;00:00, 170.05it/s, loss=0.712, val_loss=0.692, train_loss=0.718]
Epoch 1:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:00&lt;00:00, 168.60it/s, loss=0.712, val_loss=0.692, train_loss=0.718]
Epoch 1:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:00&lt;00:00, 167.60it/s, loss=0.711, val_loss=0.692, train_loss=0.718]
Epoch 1:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:00&lt;00:00, 166.88it/s, loss=0.711, val_loss=0.692, train_loss=0.718]
Epoch 1:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:00&lt;00:00, 165.87it/s, loss=0.709, val_loss=0.692, train_loss=0.718]
Epoch 1:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:00&lt;00:00, 167.09it/s, loss=0.709, val_loss=0.692, train_loss=0.718]
Epoch 1:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:00&lt;00:00, 166.40it/s, loss=0.707, val_loss=0.692, train_loss=0.718]
Epoch 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:00&lt;00:00, 166.41it/s, loss=0.707, val_loss=0.692, train_loss=0.718]
Epoch 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:00&lt;00:00, 165.82it/s, loss=0.708, val_loss=0.692, train_loss=0.718]
Epoch 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [00:00&lt;00:00, 167.29it/s, loss=0.708, val_loss=0.692, train_loss=0.718]
Epoch 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [00:00&lt;00:00, 166.82it/s, loss=0.705, val_loss=0.692, train_loss=0.718]
Epoch 1:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:00&lt;00:00, 167.97it/s, loss=0.705, val_loss=0.692, train_loss=0.718]
Epoch 1:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:00&lt;00:00, 167.54it/s, loss=0.702, val_loss=0.692, train_loss=0.718]
Epoch 1:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:00&lt;00:00, 168.79it/s, loss=0.702, val_loss=0.692, train_loss=0.718]
Epoch 1:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:00&lt;00:00, 168.35it/s, loss=0.7, val_loss=0.692, train_loss=0.718]
Epoch 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:00&lt;00:00, 169.31it/s, loss=0.7, val_loss=0.692, train_loss=0.718]
Epoch 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:00&lt;00:00, 168.95it/s, loss=0.698, val_loss=0.692, train_loss=0.718]
Epoch 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:00&lt;00:00, 169.58it/s, loss=0.698, val_loss=0.692, train_loss=0.718]
Epoch 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:00&lt;00:00, 169.25it/s, loss=0.696, val_loss=0.692, train_loss=0.718]
Epoch 1:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:00&lt;00:00, 176.87it/s, loss=0.696, val_loss=0.692, train_loss=0.718]
Epoch 1:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:00&lt;00:00, 186.45it/s, loss=0.696, val_loss=0.692, train_loss=0.718]
Epoch 1:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:00&lt;00:00, 196.06it/s, loss=0.696, val_loss=0.692, train_loss=0.718]
Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 205.81it/s, loss=0.696, val_loss=0.692, train_loss=0.718]
Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 203.23it/s, loss=0.696, val_loss=0.677, train_loss=0.718]
Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 202.28it/s, loss=0.696, val_loss=0.677, train_loss=0.690]
Epoch 1:   0%|          | 0/17 [00:00&lt;?, ?it/s, loss=0.696, val_loss=0.677, train_loss=0.690]
Epoch 2:   0%|          | 0/17 [00:00&lt;?, ?it/s, loss=0.696, val_loss=0.677, train_loss=0.690]
Epoch 2:   6%|â–Œ         | 1/17 [00:00&lt;00:00, 173.07it/s, loss=0.696, val_loss=0.677, train_loss=0.690]
Epoch 2:   6%|â–Œ         | 1/17 [00:00&lt;00:00, 168.35it/s, loss=0.693, val_loss=0.677, train_loss=0.690]
Epoch 2:  12%|â–ˆâ–        | 2/17 [00:00&lt;00:00, 175.76it/s, loss=0.693, val_loss=0.677, train_loss=0.690]
Epoch 2:  12%|â–ˆâ–        | 2/17 [00:00&lt;00:00, 173.30it/s, loss=0.694, val_loss=0.677, train_loss=0.690]
Epoch 2:  18%|â–ˆâ–Š        | 3/17 [00:00&lt;00:00, 175.01it/s, loss=0.694, val_loss=0.677, train_loss=0.690]
Epoch 2:  18%|â–ˆâ–Š        | 3/17 [00:00&lt;00:00, 173.28it/s, loss=0.688, val_loss=0.677, train_loss=0.690]
Epoch 2:  24%|â–ˆâ–ˆâ–       | 4/17 [00:00&lt;00:00, 174.09it/s, loss=0.688, val_loss=0.677, train_loss=0.690]
Epoch 2:  24%|â–ˆâ–ˆâ–       | 4/17 [00:00&lt;00:00, 172.89it/s, loss=0.687, val_loss=0.677, train_loss=0.690]
Epoch 2:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:00&lt;00:00, 173.45it/s, loss=0.687, val_loss=0.677, train_loss=0.690]
Epoch 2:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:00&lt;00:00, 172.47it/s, loss=0.684, val_loss=0.677, train_loss=0.690]
Epoch 2:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:00&lt;00:00, 172.71it/s, loss=0.684, val_loss=0.677, train_loss=0.690]
Epoch 2:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:00&lt;00:00, 171.85it/s, loss=0.681, val_loss=0.677, train_loss=0.690]
Epoch 2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:00&lt;00:00, 173.69it/s, loss=0.681, val_loss=0.677, train_loss=0.690]
Epoch 2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:00&lt;00:00, 172.88it/s, loss=0.682, val_loss=0.677, train_loss=0.690]
Epoch 2:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:00&lt;00:00, 174.00it/s, loss=0.682, val_loss=0.677, train_loss=0.690]
Epoch 2:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:00&lt;00:00, 173.33it/s, loss=0.68, val_loss=0.677, train_loss=0.690]
Epoch 2:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [00:00&lt;00:00, 171.54it/s, loss=0.68, val_loss=0.677, train_loss=0.690]
Epoch 2:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [00:00&lt;00:00, 171.02it/s, loss=0.679, val_loss=0.677, train_loss=0.690]
Epoch 2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:00&lt;00:00, 171.48it/s, loss=0.679, val_loss=0.677, train_loss=0.690]
Epoch 2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:00&lt;00:00, 170.89it/s, loss=0.677, val_loss=0.677, train_loss=0.690]
Epoch 2:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:00&lt;00:00, 170.16it/s, loss=0.677, val_loss=0.677, train_loss=0.690]
Epoch 2:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:00&lt;00:00, 169.68it/s, loss=0.675, val_loss=0.677, train_loss=0.690]
Epoch 2:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:00&lt;00:00, 170.69it/s, loss=0.675, val_loss=0.677, train_loss=0.690]
Epoch 2:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:00&lt;00:00, 170.29it/s, loss=0.673, val_loss=0.677, train_loss=0.690]
Epoch 2:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:00&lt;00:00, 170.48it/s, loss=0.673, val_loss=0.677, train_loss=0.690]
Epoch 2:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:00&lt;00:00, 170.13it/s, loss=0.672, val_loss=0.677, train_loss=0.690]
Epoch 2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:00&lt;00:00, 176.71it/s, loss=0.672, val_loss=0.677, train_loss=0.690]
Epoch 2:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:00&lt;00:00, 186.58it/s, loss=0.672, val_loss=0.677, train_loss=0.690]
Epoch 2:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:00&lt;00:00, 196.20it/s, loss=0.672, val_loss=0.677, train_loss=0.690]
Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 205.57it/s, loss=0.672, val_loss=0.677, train_loss=0.690]
Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 202.52it/s, loss=0.672, val_loss=0.651, train_loss=0.690]
Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 201.49it/s, loss=0.672, val_loss=0.651, train_loss=0.665]
Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 188.21it/s, loss=0.672, val_loss=0.651, train_loss=0.665]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     Validate metric           DataLoader 0
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   binary_accuracy_val      0.6200000047683716
    binary_auroc_val        0.7784023284912109
        val_loss            0.6511356830596924
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Training: 0it [00:00, ?it/s]
Training:   0%|          | 0/17 [00:00&lt;?, ?it/s]
Epoch 0:   0%|          | 0/17 [00:00&lt;?, ?it/s]
Epoch 0:   6%|â–Œ         | 1/17 [00:00&lt;00:00, 131.32it/s]
Epoch 0:   6%|â–Œ         | 1/17 [00:00&lt;00:00, 128.06it/s, loss=0.647]
Epoch 0:  12%|â–ˆâ–        | 2/17 [00:00&lt;00:00, 145.26it/s, loss=0.647]
Epoch 0:  12%|â–ˆâ–        | 2/17 [00:00&lt;00:00, 143.73it/s, loss=0.669]
Epoch 0:  18%|â–ˆâ–Š        | 3/17 [00:00&lt;00:00, 156.46it/s, loss=0.669]
Epoch 0:  18%|â–ˆâ–Š        | 3/17 [00:00&lt;00:00, 155.09it/s, loss=0.687]
Epoch 0:  24%|â–ˆâ–ˆâ–       | 4/17 [00:00&lt;00:00, 160.12it/s, loss=0.687]
Epoch 0:  24%|â–ˆâ–ˆâ–       | 4/17 [00:00&lt;00:00, 158.92it/s, loss=0.699]
Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:00&lt;00:00, 160.17it/s, loss=0.699]
Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:00&lt;00:00, 159.33it/s, loss=0.709]
Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:00&lt;00:00, 163.55it/s, loss=0.709]
Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:00&lt;00:00, 162.86it/s, loss=0.721]
Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:00&lt;00:00, 165.28it/s, loss=0.721]
Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:00&lt;00:00, 164.67it/s, loss=0.718]
Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:00&lt;00:00, 166.35it/s, loss=0.718]
Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:00&lt;00:00, 165.80it/s, loss=0.722]
Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [00:00&lt;00:00, 167.37it/s, loss=0.722]
Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [00:00&lt;00:00, 166.89it/s, loss=0.718]
Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:00&lt;00:00, 168.02it/s, loss=0.718]
Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:00&lt;00:00, 167.58it/s, loss=0.716]
Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:00&lt;00:00, 168.85it/s, loss=0.716]
Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:00&lt;00:00, 168.47it/s, loss=0.716]
Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:00&lt;00:00, 169.08it/s, loss=0.716]
Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:00&lt;00:00, 168.66it/s, loss=0.715]
Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:00&lt;00:00, 170.15it/s, loss=0.715]
Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:00&lt;00:00, 169.77it/s, loss=0.716]
Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:00&lt;00:00, 177.61it/s, loss=0.716]
Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:00&lt;00:00, 187.34it/s, loss=0.716]
Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:00&lt;00:00, 197.01it/s, loss=0.716]
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 207.06it/s, loss=0.716]
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 203.03it/s, loss=0.716, val_loss=0.701]
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 202.09it/s, loss=0.716, val_loss=0.701, train_loss=0.715]
Epoch 0:   0%|          | 0/17 [00:00&lt;?, ?it/s, loss=0.716, val_loss=0.701, train_loss=0.715]
Epoch 1:   0%|          | 0/17 [00:00&lt;?, ?it/s, loss=0.716, val_loss=0.701, train_loss=0.715]
Epoch 1:   6%|â–Œ         | 1/17 [00:00&lt;00:00, 155.45it/s, loss=0.716, val_loss=0.701, train_loss=0.715]
Epoch 1:   6%|â–Œ         | 1/17 [00:00&lt;00:00, 151.42it/s, loss=0.714, val_loss=0.701, train_loss=0.715]
Epoch 1:  12%|â–ˆâ–        | 2/17 [00:00&lt;00:00, 163.96it/s, loss=0.714, val_loss=0.701, train_loss=0.715]
Epoch 1:  12%|â–ˆâ–        | 2/17 [00:00&lt;00:00, 161.83it/s, loss=0.713, val_loss=0.701, train_loss=0.715]
Epoch 1:  18%|â–ˆâ–Š        | 3/17 [00:00&lt;00:00, 169.89it/s, loss=0.713, val_loss=0.701, train_loss=0.715]
Epoch 1:  18%|â–ˆâ–Š        | 3/17 [00:00&lt;00:00, 168.03it/s, loss=0.711, val_loss=0.701, train_loss=0.715]
Epoch 1:  24%|â–ˆâ–ˆâ–       | 4/17 [00:00&lt;00:00, 169.49it/s, loss=0.711, val_loss=0.701, train_loss=0.715]
Epoch 1:  24%|â–ˆâ–ˆâ–       | 4/17 [00:00&lt;00:00, 168.34it/s, loss=0.71, val_loss=0.701, train_loss=0.715]
Epoch 1:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:00&lt;00:00, 172.83it/s, loss=0.71, val_loss=0.701, train_loss=0.715]
Epoch 1:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:00&lt;00:00, 171.89it/s, loss=0.708, val_loss=0.701, train_loss=0.715]
Epoch 1:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:00&lt;00:00, 171.82it/s, loss=0.708, val_loss=0.701, train_loss=0.715]
Epoch 1:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:00&lt;00:00, 170.99it/s, loss=0.708, val_loss=0.701, train_loss=0.715]
Epoch 1:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:00&lt;00:00, 173.04it/s, loss=0.708, val_loss=0.701, train_loss=0.715]
Epoch 1:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:00&lt;00:00, 172.19it/s, loss=0.707, val_loss=0.701, train_loss=0.715]
Epoch 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:00&lt;00:00, 171.57it/s, loss=0.707, val_loss=0.701, train_loss=0.715]
Epoch 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:00&lt;00:00, 170.91it/s, loss=0.708, val_loss=0.701, train_loss=0.715]
Epoch 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [00:00&lt;00:00, 170.71it/s, loss=0.708, val_loss=0.701, train_loss=0.715]
Epoch 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [00:00&lt;00:00, 170.17it/s, loss=0.708, val_loss=0.701, train_loss=0.715]
Epoch 1:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:00&lt;00:00, 172.03it/s, loss=0.708, val_loss=0.701, train_loss=0.715]
Epoch 1:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:00&lt;00:00, 171.49it/s, loss=0.707, val_loss=0.701, train_loss=0.715]
Epoch 1:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:00&lt;00:00, 172.29it/s, loss=0.707, val_loss=0.701, train_loss=0.715]
Epoch 1:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:00&lt;00:00, 171.84it/s, loss=0.704, val_loss=0.701, train_loss=0.715]
Epoch 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:00&lt;00:00, 172.66it/s, loss=0.704, val_loss=0.701, train_loss=0.715]
Epoch 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:00&lt;00:00, 172.28it/s, loss=0.701, val_loss=0.701, train_loss=0.715]
Epoch 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:00&lt;00:00, 172.02it/s, loss=0.701, val_loss=0.701, train_loss=0.715]
Epoch 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:00&lt;00:00, 171.64it/s, loss=0.696, val_loss=0.701, train_loss=0.715]
Epoch 1:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:00&lt;00:00, 179.03it/s, loss=0.696, val_loss=0.701, train_loss=0.715]
Epoch 1:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:00&lt;00:00, 189.05it/s, loss=0.696, val_loss=0.701, train_loss=0.715]
Epoch 1:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:00&lt;00:00, 198.91it/s, loss=0.696, val_loss=0.701, train_loss=0.715]
Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 209.11it/s, loss=0.696, val_loss=0.701, train_loss=0.715]
Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 206.24it/s, loss=0.696, val_loss=0.682, train_loss=0.715]
Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 205.23it/s, loss=0.696, val_loss=0.682, train_loss=0.688]
Epoch 1:   0%|          | 0/17 [00:00&lt;?, ?it/s, loss=0.696, val_loss=0.682, train_loss=0.688]
Epoch 2:   0%|          | 0/17 [00:00&lt;?, ?it/s, loss=0.696, val_loss=0.682, train_loss=0.688]
Epoch 2:   6%|â–Œ         | 1/17 [00:00&lt;00:00, 170.01it/s, loss=0.696, val_loss=0.682, train_loss=0.688]
Epoch 2:   6%|â–Œ         | 1/17 [00:00&lt;00:00, 165.31it/s, loss=0.694, val_loss=0.682, train_loss=0.688]
Epoch 2:  12%|â–ˆâ–        | 2/17 [00:00&lt;00:00, 170.17it/s, loss=0.694, val_loss=0.682, train_loss=0.688]
Epoch 2:  12%|â–ˆâ–        | 2/17 [00:00&lt;00:00, 167.97it/s, loss=0.69, val_loss=0.682, train_loss=0.688]
Epoch 2:  18%|â–ˆâ–Š        | 3/17 [00:00&lt;00:00, 172.54it/s, loss=0.69, val_loss=0.682, train_loss=0.688]
Epoch 2:  18%|â–ˆâ–Š        | 3/17 [00:00&lt;00:00, 170.88it/s, loss=0.688, val_loss=0.682, train_loss=0.688]
Epoch 2:  24%|â–ˆâ–ˆâ–       | 4/17 [00:00&lt;00:00, 173.38it/s, loss=0.688, val_loss=0.682, train_loss=0.688]
Epoch 2:  24%|â–ˆâ–ˆâ–       | 4/17 [00:00&lt;00:00, 172.05it/s, loss=0.688, val_loss=0.682, train_loss=0.688]
Epoch 2:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:00&lt;00:00, 171.97it/s, loss=0.688, val_loss=0.682, train_loss=0.688]
Epoch 2:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:00&lt;00:00, 170.96it/s, loss=0.684, val_loss=0.682, train_loss=0.688]
Epoch 2:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:00&lt;00:00, 171.88it/s, loss=0.684, val_loss=0.682, train_loss=0.688]
Epoch 2:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:00&lt;00:00, 171.07it/s, loss=0.685, val_loss=0.682, train_loss=0.688]
Epoch 2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:00&lt;00:00, 171.13it/s, loss=0.685, val_loss=0.682, train_loss=0.688]
Epoch 2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [00:00&lt;00:00, 170.44it/s, loss=0.681, val_loss=0.682, train_loss=0.688]
Epoch 2:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:00&lt;00:00, 171.42it/s, loss=0.681, val_loss=0.682, train_loss=0.688]
Epoch 2:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [00:00&lt;00:00, 170.80it/s, loss=0.682, val_loss=0.682, train_loss=0.688]
Epoch 2:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [00:00&lt;00:00, 171.77it/s, loss=0.682, val_loss=0.682, train_loss=0.688]
Epoch 2:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [00:00&lt;00:00, 171.23it/s, loss=0.68, val_loss=0.682, train_loss=0.688]
Epoch 2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:00&lt;00:00, 172.80it/s, loss=0.68, val_loss=0.682, train_loss=0.688]
Epoch 2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [00:00&lt;00:00, 172.37it/s, loss=0.681, val_loss=0.682, train_loss=0.688]
Epoch 2:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:00&lt;00:00, 173.29it/s, loss=0.681, val_loss=0.682, train_loss=0.688]
Epoch 2:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [00:00&lt;00:00, 172.86it/s, loss=0.677, val_loss=0.682, train_loss=0.688]
Epoch 2:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:00&lt;00:00, 173.61it/s, loss=0.677, val_loss=0.682, train_loss=0.688]
Epoch 2:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [00:00&lt;00:00, 173.23it/s, loss=0.677, val_loss=0.682, train_loss=0.688]
Epoch 2:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:00&lt;00:00, 173.68it/s, loss=0.677, val_loss=0.682, train_loss=0.688]
Epoch 2:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [00:00&lt;00:00, 173.35it/s, loss=0.676, val_loss=0.682, train_loss=0.688]
Epoch 2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [00:00&lt;00:00, 180.98it/s, loss=0.676, val_loss=0.682, train_loss=0.688]
Epoch 2:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [00:00&lt;00:00, 191.05it/s, loss=0.676, val_loss=0.682, train_loss=0.688]
Epoch 2:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:00&lt;00:00, 200.95it/s, loss=0.676, val_loss=0.682, train_loss=0.688]
Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 211.17it/s, loss=0.676, val_loss=0.682, train_loss=0.688]
Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 208.40it/s, loss=0.676, val_loss=0.652, train_loss=0.688]
Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 207.41it/s, loss=0.676, val_loss=0.652, train_loss=0.671]
Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:00&lt;00:00, 193.41it/s, loss=0.676, val_loss=0.652, train_loss=0.671]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     Validate metric           DataLoader 0
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   binary_accuracy_val      0.5099999904632568
    binary_auroc_val        0.8143256902694702
        val_loss            0.6516168117523193
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
</pre></div>
</div>
</section>
<section id="plot-the-results">
<h2>6. Plot the results<a class="headerlink" href="#plot-the-results" title="Permalink to this heading">ïƒ</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plotter</span> <span class="o">=</span> <span class="n">Plotter</span><span class="p">(</span><span class="n">single_model_dict</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
<span class="n">single_model_figures_dict</span> <span class="o">=</span> <span class="n">plotter</span><span class="o">.</span><span class="n">plot_all</span><span class="p">()</span>
<span class="n">plotter</span><span class="o">.</span><span class="n">show_all</span><span class="p">(</span><span class="n">single_model_figures_dict</span><span class="p">)</span>
</pre></div>
</div>
<ul class="sphx-glr-horizontal">
<li><img src="../_images/sphx_glr_plot_one_model_binary_kfold_001.png" srcset="../_images/sphx_glr_plot_one_model_binary_kfold_001.png" alt="TabularCrossmodalMultiheadAttention: confusion matrices, Fold 1 binary_auroc:  0.814, Fold 2 binary_auroc:  0.744, Fold 3 binary_auroc:  0.813, Fold 4 binary_auroc:  0.778, Fold 5 binary_auroc:  0.814" class = "sphx-glr-multi-img"/></li>
<li><img src="../_images/sphx_glr_plot_one_model_binary_kfold_002.png" srcset="../_images/sphx_glr_plot_one_model_binary_kfold_002.png" alt="TabularCrossmodalMultiheadAttention: binary_auroc = 0.784" class = "sphx-glr-multi-img"/></li>
</ul>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Plotting models [&#39;TabularCrossmodalMultiheadAttention&#39;] ...
Plotting results of a single model.
TabularCrossmodalMultiheadAttention_confusion_matrix_kfold
TabularCrossmodalMultiheadAttention_confusion_matrix_kfold_together
</pre></div>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 2.210 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-examples-plot-one-model-binary-kfold-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/e5c70abf52c8b0b89b0a63287f3727d4/plot_one_model_binary_kfold.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_one_model_binary_kfold.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/5b4883812463c56665c4037b7e4fa86a/plot_one_model_binary_kfold.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_one_model_binary_kfold.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Florence J Townend.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>